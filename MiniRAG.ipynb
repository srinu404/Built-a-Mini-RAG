{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKqWB4rmdaab",
        "outputId": "f60a1367-f75f-414b-e0b9-e35f80ddd5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 1s\n",
            "\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "# @title 1. Install Dependencies\n",
        "!pip install -q langchain langchain-community langchain-text-splitters faiss-cpu sentence-transformers pypdf google-generativeai streamlit\n",
        "!npm install localtunnel # Required to expose Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Create Vector Store (Ingestion)\n",
        "import os\n",
        "import shutil\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "def run_ingestion():\n",
        "    # 1. Setup Data Folder\n",
        "    if os.path.exists(\"data\"):\n",
        "        shutil.rmtree(\"data\")\n",
        "    os.makedirs(\"data\")\n",
        "\n",
        "    # Move uploaded files to data folder\n",
        "    uploaded_files = [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]\n",
        "    found_files = []\n",
        "\n",
        "    for filename in uploaded_files:\n",
        "        if os.path.exists(filename):\n",
        "            shutil.move(filename, f\"data/{filename}\")\n",
        "            found_files.append(filename)\n",
        "\n",
        "    if not found_files:\n",
        "        print(\"âŒ Error: No PDF files found! Please drag and drop doc1.pdf, doc2.pdf, and doc3.pdf into the file pane.\")\n",
        "        return\n",
        "\n",
        "    print(f\"âœ… Found and moved {len(found_files)} files to 'data/' folder.\")\n",
        "\n",
        "    # 2. Load PDFs\n",
        "    print(\"Loading documents...\")\n",
        "    loader = PyPDFDirectoryLoader(\"data\")\n",
        "    documents = loader.load()\n",
        "    print(f\"Loaded {len(documents)} pages.\")\n",
        "\n",
        "    # 3. Split Text\n",
        "    print(\"Splitting text into chunks...\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=50,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Created {len(chunks)} chunks.\")\n",
        "\n",
        "    # 4. Create Embeddings & Vector Store\n",
        "    print(\"Creating vector store (this takes ~1 minute)...\")\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                       model_kwargs={'device': 'cpu'})\n",
        "\n",
        "    db = FAISS.from_documents(chunks, embeddings)\n",
        "    db.save_local(\"vector_store/db_faiss\")\n",
        "    print(\"âœ… Vector store saved successfully!\")\n",
        "\n",
        "# Run the ingestion\n",
        "run_ingestion()"
      ],
      "metadata": {
        "id": "WQHdmYj-dhky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c811778-3e42-4f4d-8010-a6a70bf17b4b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Found and moved 3 files to 'data/' folder.\n",
            "Loading documents...\n",
            "Loaded 10 pages.\n",
            "Splitting text into chunks...\n",
            "Created 30 chunks.\n",
            "Creating vector store (this takes ~1 minute)...\n",
            "âœ… Vector store saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Diagnostic: Check Available Models\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# 1. Enter your API Key here for testing\n",
        "os.environ[\"GOOGLE_API_KEY\"] = input(\"Enter your Gemini API Key: \")\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "print(\"\\nChecking available models...\")\n",
        "try:\n",
        "    for m in genai.list_models():\n",
        "        if 'generateContent' in m.supported_generation_methods:\n",
        "            print(f\"- {m.name}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error: {e}\")\n",
        "    print(\"Please check if your API Key is valid and enabled in Google AI Studio.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "kK-ZYvsPxyyp",
        "outputId": "48d8c374-afaf-403a-c618-33635086202d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Gemini API Key: AIzaSyC639a92Aww6kpMenFl1MCBcQ0Y5GqWkqU\n",
            "\n",
            "Checking available models...\n",
            "- models/gemini-2.5-flash\n",
            "- models/gemini-2.5-pro\n",
            "- models/gemini-2.0-flash-exp\n",
            "- models/gemini-2.0-flash\n",
            "- models/gemini-2.0-flash-001\n",
            "- models/gemini-2.0-flash-exp-image-generation\n",
            "- models/gemini-2.0-flash-lite-001\n",
            "- models/gemini-2.0-flash-lite\n",
            "- models/gemini-2.0-flash-lite-preview-02-05\n",
            "- models/gemini-2.0-flash-lite-preview\n",
            "- models/gemini-exp-1206\n",
            "- models/gemini-2.5-flash-preview-tts\n",
            "- models/gemini-2.5-pro-preview-tts\n",
            "- models/gemma-3-1b-it\n",
            "- models/gemma-3-4b-it\n",
            "- models/gemma-3-12b-it\n",
            "- models/gemma-3-27b-it\n",
            "- models/gemma-3n-e4b-it\n",
            "- models/gemma-3n-e2b-it\n",
            "- models/gemini-flash-latest\n",
            "- models/gemini-flash-lite-latest\n",
            "- models/gemini-pro-latest\n",
            "- models/gemini-2.5-flash-lite\n",
            "- models/gemini-2.5-flash-image-preview\n",
            "- models/gemini-2.5-flash-image\n",
            "- models/gemini-2.5-flash-preview-09-2025\n",
            "- models/gemini-2.5-flash-lite-preview-09-2025\n",
            "- models/gemini-3-pro-preview\n",
            "- models/gemini-3-flash-preview\n",
            "- models/gemini-3-pro-image-preview\n",
            "- models/nano-banana-pro-preview\n",
            "- models/gemini-robotics-er-1.5-preview\n",
            "- models/gemini-2.5-computer-use-preview-10-2025\n",
            "- models/deep-research-pro-preview-12-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import time\n",
        "\n",
        "# --- Configuration ---\n",
        "st.set_page_config(page_title=\"Indecimal Assistant\", page_icon=\"ðŸ—ï¸\")\n",
        "DB_FAISS_PATH = \"vector_store/db_faiss\"\n",
        "\n",
        "# --- UI Layout ---\n",
        "st.title(\"ðŸ—ï¸ Indecimal Site Assistant\")\n",
        "\n",
        "# Sidebar for Settings\n",
        "with st.sidebar:\n",
        "    st.header(\"Settings\")\n",
        "    api_key = st.text_input(\"Enter Gemini API Key\", type=\"password\")\n",
        "\n",
        "    # Model Selection\n",
        "    model_name = st.selectbox(\n",
        "        \"Select Model\",\n",
        "        [\"gemini-flash-latest\", \"gemini-2.0-flash-lite\", \"gemini-pro-latest\"],\n",
        "        index=0\n",
        "    )\n",
        "\n",
        "    if api_key:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "        genai.configure(api_key=api_key)\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"**Status:** Ready\")\n",
        "\n",
        "# --- Load Vector Store ---\n",
        "@st.cache_resource\n",
        "def get_vector_store():\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                       model_kwargs={'device': 'cpu'})\n",
        "    if os.path.exists(DB_FAISS_PATH):\n",
        "        db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "        return db\n",
        "    return None\n",
        "\n",
        "# --- Main RAG Logic ---\n",
        "def get_response(query, db, selected_model):\n",
        "    docs = db.similarity_search(query, k=3)\n",
        "    context_text = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "        return \"âš ï¸ Please enter your API Key in the sidebar.\", []\n",
        "\n",
        "    model = genai.GenerativeModel(selected_model)\n",
        "\n",
        "    # --- POLITE PROMPT ---\n",
        "    prompt = f\"\"\"\n",
        "    You are a polite and helpful assistant for Indecimal, a construction marketplace.\n",
        "    Answer the user's question based ONLY on the following context.\n",
        "\n",
        "    Context:\n",
        "    {context_text}\n",
        "\n",
        "    Question:\n",
        "    {query}\n",
        "\n",
        "    Instructions:\n",
        "    - If the answer is about pricing, quote the exact amount.\n",
        "    - If the answer is NOT in the context, politely say:\n",
        "      \"I apologize, but I couldn't find that specific detail in the available documents. I will check with the team and get back to you.\"\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text, docs\n",
        "    except Exception as e:\n",
        "        if \"429\" in str(e):\n",
        "            return \"âš ï¸ **Rate Limit Hit:** Please wait 30 seconds or try 'gemini-2.0-flash-lite' in the sidebar.\", []\n",
        "        return f\"Error with model '{selected_model}': {str(e)}\", []\n",
        "\n",
        "# --- Chat Interface ---\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"Ask about Indecimal packages...\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    db = get_vector_store()\n",
        "    if db:\n",
        "        with st.spinner(f\"Thinking using {model_name}...\"):\n",
        "            response_text, source_docs = get_response(prompt, db, model_name)\n",
        "\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(response_text)\n",
        "\n",
        "            # --- REMOVED THE RETRIEVED CONTEXT EXPANDER HERE ---\n",
        "\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response_text})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHmBaw-nf6SZ",
        "outputId": "dd76113a-8a8b-4906-e3b2-2584ef072df6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Run Streamlit (Cloudflare Tunnel)\n",
        "import time\n",
        "import os\n",
        "\n",
        "# 1. Kill old processes to ensure a clean start\n",
        "!pkill -9 -f streamlit\n",
        "!pkill -9 -f cloudflared\n",
        "\n",
        "# 2. Download Cloudflare Tunnel (if not already there)\n",
        "if not os.path.exists(\"cloudflared\"):\n",
        "    !wget -q -O cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "    !chmod +x cloudflared\n",
        "\n",
        "# 3. Run Streamlit in the background\n",
        "print(\"Starting Streamlit...\")\n",
        "!streamlit run app.py >/dev/null 2>&1 &\n",
        "time.sleep(3) # Give Streamlit a moment to start\n",
        "\n",
        "# 4. Start the Tunnel\n",
        "print(\"Initializing Cloudflare Tunnel...\")\n",
        "# We redirect output to a log file so we can extract the URL cleanly\n",
        "!./cloudflared tunnel --url http://localhost:8501 > cloudflared.log 2>&1 &\n",
        "\n",
        "# 5. Extract and Print the URL\n",
        "print(\"Waiting for link...\", end=\"\")\n",
        "for _ in range(10):\n",
        "    time.sleep(1)\n",
        "    print(\".\", end=\"\")\n",
        "\n",
        "print(\"\\n\\nðŸ‘‡ CLICK THIS LINK TO OPEN YOUR APP ðŸ‘‡\")\n",
        "!grep -o 'https://.*\\.trycloudflare.com' cloudflared.log | head -n 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfsHIoQuiwAh",
        "outputId": "8b907cb1-3e00-4b51-d9a4-ac6c0c5a80f3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Streamlit...\n",
            "Initializing Cloudflare Tunnel...\n",
            "Waiting for link.............\n",
            "\n",
            "ðŸ‘‡ CLICK THIS LINK TO OPEN YOUR APP ðŸ‘‡\n",
            "https://funny-soma-medal-being.trycloudflare.com\n"
          ]
        }
      ]
    }
  ]
}